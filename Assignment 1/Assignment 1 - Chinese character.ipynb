{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3844c336",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "Optical character recognition is an old \"AI\" and image-processing task.  What it involves is taking a photograph or scan of a piece of text (printed or handwritten) and turning the characters (as images) into character codes on the computer that therefore allow the text to be edited, indexed, etc.  A key part of that process is identifying where the characters actually are, especially if the characters are mixed among other non-writing, such as images of objects or people.\n",
    "\n",
    "In this assignment, you will take images from a Chinese image database with annotations that indicate where the Chinese characters are, and you will train a model that takes test images, and superimposes upon them a visualization (of your choosing, e.g., a \"heat map\") of the likelihood that a pixel is close to or part of a valid Chinese character.  The image database contains annotations of \"bounding boxes\", coordinates of the corners of a box that contains a single Chinese character.  In a sense, this assignment asks you to detect the bounding boxes in test images without the annotation, but a softer version of this: simply to provide the probability, for each pixel, whether that pixel was part of a bounding box containing a Chinese character.  Then, you are to (1) superimpose upon the image a pixel-based map of likelihoods of where the bounding boxes ought to be and (2) apply an evaluation statistic.\n",
    "\n",
    "This assignment grants you a lot of freedom in how you organize your code and set up the task overall.  Because of the degree of freedom it involves, it will mostly be graded on our evaluation of the effort put into the solution.  An actual high success at the task is not a requirement to get a high grade.  However, you will have to report in detail, in your own format, what you did, why you did it, how to run it -- it must run on mltgpu, be implemented in Python using PyTorch, and make use of the GPUs -- and how to apply it easily to our own test images.\n",
    "\n",
    "You will have almost a month to do this assignment, even though it is worth only 30% of your grade.  Another assignment with 30% will be given out for the last/remaining two weeks of the study period.   These time periods are coextensive with that of the project, but we expect you to be able to schedule your time well enough to put in an effort at both. This assignment is officially due at **23:59 on 2021 October 18**. There are 30 points on this assignment, and a maximum of 20 bonus points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f26b8",
   "metadata": {},
   "source": [
    "# **The data**\n",
    "\n",
    "The source of the task is here: https://ctwdataset.github.io/ (Links to an external site.) They have example images and an example of a baseline task that is much more advanced than what we are doing, but it will give you an idea of the data format, particularly the metadata.  Pay attention especially to the \"Annotation format\" section of this page: https://ctwdataset.github.io/tutorial/1-basics.html (Links to an external site.)\n",
    "\n",
    "The metadata and a small sample of the whole image dataset is available at /scratch/lt2326-h21/a1 on mltgpu. The metadata is in json format.  info.json contains information about every image file.  We will unzip only a minority of the original training image files.  train.jsonl is a list of json entities, one per line (that have to be parsed with the json package each separately) that correspond to the files in info.json.  This contains the bounding box information, as well as other information for the original challenge on the web.  See the \"Annotation format\" section mentioned on the dataset web page linked above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87fc53a",
   "metadata": {},
   "source": [
    "# **Part 1: data preparation (7 points)**\n",
    "\n",
    "The image files are in /scratch/lt2326-h21/a1/images on mltgpu. They are in jpg format.  The code that you write for this part of the project should:\n",
    "\n",
    "- Use the info.json file to figure out what files are in the training set.  You will just use the official training data for everything.  Remember that you will only see a small minority of training examples in the images directory, for space reasons.\n",
    "- Divide up the official training data files into your own training, validation, and test datasets depending on your own preferences. You can choose to use fewer files than the maximum available if you run into problems with memory and so on (but first make sure your implementation is reasonably efficient).\n",
    "- Find the corresponding bounding box information in train.jsonl for each image.\n",
    "\n",
    "You can represent the data in any way you like, but remember that it will become a numpy array for processing and a torch tensor for training.  Remember also that the classes are defined by pixel: for each pixel, you will eventually have a set of features (e.g. colour values), and a binary class corresponding to whether the pixel was in a Chinese character bounding box or not (note that there are non-Chinese characters in the set -- see the annotation instructions).  You are allowed to reduce the dimensionality of the images for processing, but consider using a pooling and/or upsampling technique in Part 2 of this assignment to accomplish this goal. \n",
    "\n",
    "Describe the choices you made and the challenges you found in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a460d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch import nn\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.path as mplpath\n",
    "from skimage import io\n",
    "\n",
    "#device\n",
    "device = torch.device('cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "684c563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "\n",
    "images_directory = \"/scratch/lt2326-h21/a1/images\"\n",
    "train_json = \"/scratch/lt2326-h21/a1/train.jsonl\"\n",
    "info_json = \"/scratch/lt2326-h21/a1/info.json\"\n",
    "\n",
    "\n",
    "#opening files and images\n",
    "\n",
    "with open(train_json) as trainfile:\n",
    "    train_data = [json.loads(x) for x in trainfile]\n",
    "\n",
    "with open(info_json) as infofile:\n",
    "    info_data = json.load(infofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "439b2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out what files are in the training set\n",
    "\n",
    "selection = ['train']\n",
    "filtered = list(filter(lambda i: i[0] in selection, info_data.items()))\n",
    "filenames = [d['file_name'] for d in filtered[0][1]]\n",
    "\n",
    "iiiimages = os.listdir(images_directory)\n",
    "\n",
    "my_images = []\n",
    "for n in iiiimages:\n",
    "    if n in filenames:\n",
    "        my_images.append(n.replace('.jpg', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77083795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all usable files\n",
    "\n",
    "files = []\n",
    "for image in train_data:\n",
    "    if image['image_id'] in my_images:\n",
    "        files.append(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd558f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the corresponding bounding box information in train.jsonl for each image\n",
    "\n",
    "def chinese(images):\n",
    "    ch_dict = {}\n",
    "    \n",
    "    for i in images:\n",
    "        ch_dict[i['image_id']] = {'polygons' : []}\n",
    "        for annotation in i['annotations']:\n",
    "            for an in annotation:\n",
    "                if an['is_chinese'] == True:\n",
    "                    ch_dict[i['image_id']]['polygons'].append(an['polygon'])\n",
    "                    \n",
    "    return ch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e219dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_tensor(path_to_img):\n",
    "#     img = Image.open(path_to_img)\n",
    "#     image_array = np.array(img)\n",
    "    \n",
    "    img = io.imread(path_to_img)\n",
    "\n",
    "    img = torch.tensor(img, device=device).float()\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9b0150b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_truth(polygons_list):\n",
    "    \n",
    "    grr = [[[a, b] for b in list(range(2048))] for a in list(range(2048))]\n",
    "    grid = np.array(grr)\n",
    "    grid.shape = (4194304, 2)\n",
    "    \n",
    "    p = [pol for pol in polygons_list['polygons']]\n",
    "\n",
    "    truth_array = np.zeros(4194304)\n",
    "    for x in p:\n",
    "        p2 = mplpath.Path(x)\n",
    "        truth = np.asarray(p2.contains_points(grid), int)\n",
    "        truth_array = truth_array + truth\n",
    "    \n",
    "    return truth_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "426d3140",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_965915/2840329765.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0meverything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdata_for_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchinese_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_965915/2840329765.py\u001b[0m in \u001b[0;36mdata_for_model\u001b[0;34m(chinese_dict)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mrgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_directory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mgold_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_truth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchinese_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0meverything\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_965915/1572783978.py\u001b[0m in \u001b[0;36mget_truth\u001b[0;34m(polygons_list)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_truth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolygons_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgrr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4194304\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_965915/1572783978.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_truth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolygons_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgrr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4194304\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_965915/1572783978.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_truth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolygons_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgrr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4194304\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PLAN\n",
    "\n",
    "for every image in files (all the traindata, 845 imgs), pass them through a function that returns:\n",
    "- array of rgb values of the image\n",
    "- array of polygons truth of that image (takes info from ch_dict)\n",
    "as tuple\n",
    "\"\"\"\n",
    "\n",
    "def data_for_model(chinese_dict):\n",
    "    \n",
    "    everything = [] #list of tuples?\n",
    "    \n",
    "    for f in chinese_dict:\n",
    "        \n",
    "        rgb_array = img_to_tensor(images_directory + \"/\" + str(f) + \".jpg\")\n",
    "        \n",
    "        gold_truth = get_truth(chinese_dict[f])\n",
    "        \n",
    "        everything.append((rgb_array, gold_truth))\n",
    "        \n",
    "    return everything\n",
    "\n",
    "data_for_model(chinese_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8abbd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_for_model(chinese_dict)\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "training, validation, test = np.split(data, [int(len(files)*0.8), int(len(files)*0.9)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3b886",
   "metadata": {},
   "source": [
    "# **Part 2: the models (10 points)**\n",
    "\n",
    "In this part, you will implement two substantially different model archictectures, that both take your representation of the images as training input and both take your representation of the bounding boxes as objective (HINT: the binary classification of pixels as belonging to a bounding box or not).  They will save the trained models to files so that they can be loaded and tested later. The output of the models will be a \"soft binary\" -- the probability of each pixel being inside a bounding box, from 0 to 1.  Consider examining some of the training data before designing your architectures.\n",
    "\n",
    "You have a large grant of freedom as to what these model architectures will look like (remember: grading is on a \"reasonable effort\" basis).  There's a high chance (HINT) that they will both use one or more convolutional layers, among other things.  Describe the models and the motivations for the architecture in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860ca8f7",
   "metadata": {},
   "source": [
    "CNN for inspo: https://www.pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn/#pyis-cta-modal\n",
    "https://towardsdatascience.com/beginners-guide-to-building-convolutional-neural-networks-using-tensorflow-s-keras-api-in-python-6e8035e28238\n",
    "http://parneetk.github.io/blog/cnn-mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b3ead5",
   "metadata": {},
   "source": [
    "# **Part 3: testing and evaluation (13 points)**\n",
    "\n",
    "You can use your test data by feeding the test images forward through the models. The output of the models will be pixel maps of the probability of a particular pixel being inside a bounding box.  These will be compared outside the model to the test data's bounding boxes.  You can use a number of different evaluation strategies -- one of them being to choose a probability threshold to decide whether a pixel is inside the bounding box or not, and then take recall/precision/X11/accuracy. Another one is to report it in terms of error, such as mean squared error. Even given your architectural choices, you will likely have hyperparameters to tune.  Describe the progress of your training and testing, with graphs if necessary, in your report.\n",
    "\n",
    "It should also be possible to examine the effects of applying the model to individual images.  Make it possible to visually represent the pixel/bounding box probabilities superimposed on the original images.  Examine some of the images to conduct a qualitative error analysis of your trained models. Include this analysis in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68713450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
